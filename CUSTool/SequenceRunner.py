#!/usr/bin/env python3
"""
SequenceRunner - Intelligent Test Sequence Execution Manager

This module manages the execution of test sequences generated by TestCaseCreator.
It handles sequence loading, execution, state management, and comprehensive reporting.

Features:
- Load and execute test sequences
- State management and progression tracking
- Coverage verification and reporting
- Adaptive sequence selection based on current program state
- Real-time monitoring and adjustment
- Comprehensive logging and metrics
- AI-optimized defect prompt generation on failures
"""

import os
import json
import time
import random
from typing import Dict, List, Optional, Tuple, Set
from dataclasses import dataclass, field
from enum import Enum
from datetime import datetime
import threading
import queue

# Import IssuePromptGenerator for defect reporting
try:
    from IssuePromptGenerator import IssuePromptGenerator, IssueSeverity, FailureType, TestCaseContext, DocumentationReference
    ISSUE_PROMPT_AVAILABLE = True
except ImportError:
    ISSUE_PROMPT_AVAILABLE = False
    print("Warning: IssuePromptGenerator not available - defect prompt generation disabled")

class SequenceState(Enum):
    """Sequence execution states"""
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    SKIPPED = "skipped"
    PAUSED = "paused"

class StepResult(Enum):
    """Individual step execution results"""
    SUCCESS = "success"
    FAILED = "failed"
    TIMEOUT = "timeout"
    SKIPPED = "skipped"

@dataclass
class ExecutionStep:
    """Individual step in a test sequence"""
    trigger: str
    action: str
    description: str
    expected_result: Optional[str] = None
    timeout: int = 30
    retry_count: int = 0
    max_retries: int = 3

@dataclass
@dataclass
class SequenceExecution:
    """Tracks the execution of a test sequence"""
    name: str
    description: str
    steps: List[ExecutionStep]
    sequence_id: str = ""
    state: SequenceState = SequenceState.PENDING
    current_step: int = 0
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None
    results: List[StepResult] = field(default_factory=list)
    error_messages: List[str] = field(default_factory=list)
    coverage_achieved: Set[str] = field(default_factory=set)
    priority: int = 1
    prerequisites: List[str] = field(default_factory=list)

@dataclass
class ExecutionMetrics:
    """Metrics for sequence execution"""
    total_sequences: int = 0
    completed_sequences: int = 0
    failed_sequences: int = 0
    skipped_sequences: int = 0
    total_steps: int = 0
    successful_steps: int = 0
    failed_steps: int = 0
    total_execution_time: float = 0.0
    coverage_percentage: float = 0.0

class SequenceRunner:
    """
    Manages and executes test sequences with intelligent state management
    """
    
    def __init__(self, config_file: str = "sequence_runner_config.json"):
        self.config_file = config_file
        self.config = self._load_config()
        self.sequences: List[SequenceExecution] = []
        self.metrics = ExecutionMetrics()
        self.current_sequence: Optional[SequenceExecution] = None
        self.execution_queue = queue.Queue()
        self.state_history: List[str] = []
        self.coverage_map: Dict[str, bool] = {}
        self.is_running = False
        self.pause_event = threading.Event()
        self.stop_event = threading.Event()
        
        # Initialize CUS integration
        self.cus_integration = None
        self._setup_cus_integration()
        
        # Initialize IssuePromptGenerator for defect reporting
        self.issue_prompt_generator = None
        if ISSUE_PROMPT_AVAILABLE:
            self._setup_issue_prompt_generator()
        
    def _load_config(self) -> Dict:
        """Load configuration for sequence runner"""
        default_config = {
            "execution_settings": {
                "max_concurrent_sequences": 1,
                "step_timeout": 30,
                "sequence_timeout": 300,
                "retry_failed_steps": True,
                "max_retries": 3,
                "pause_on_error": True,
                "adaptive_timing": True
            },
            "monitoring": {
                "screenshot_on_error": True,
                "log_level": "INFO",
                "metrics_interval": 60,
                "state_tracking": True
            },
            "cus_integration": {
                "cus_script_path": "CUS.py",
                "use_existing_cus": True,
                "restart_cus_on_failure": True
            },
            "reporting": {
                "generate_html_report": True,
                "save_detailed_logs": True,
                "export_metrics": True
            },
            "external_program_path": "",
            "defect_reporting": {
                "enabled": True,
                "capture_before_after": False,
                "capture_sequence": False,
                "annotate_screenshots": True,
                "auto_generate_on_failure": True
            }
        }
        
        if os.path.exists(self.config_file):
            try:
                with open(self.config_file, 'r') as f:
                    user_config = json.load(f)
                    default_config.update(user_config)
            except Exception as e:
                print(f"Error loading config: {e}, using defaults")
        
        return default_config
    
    def _setup_cus_integration(self):
        """Set up integration with CUS system"""
        try:
            # Import CUS components if available
            cus_path = self.config["cus_integration"]["cus_script_path"]
            if os.path.exists(cus_path):
                print(f"CUS integration available at: {cus_path}")
                self.cus_integration = {
                    "available": True,
                    "path": cus_path,
                    "process": None
                }
            else:
                print("CUS integration not available")
                self.cus_integration = {"available": False}
        except Exception as e:
            print(f"Error setting up CUS integration: {e}")
            self.cus_integration = {"available": False}
    
    def _setup_issue_prompt_generator(self):
        """Set up IssuePromptGenerator for defect reporting"""
        try:
            # Get external program path from config
            external_program_path = self.config.get("external_program_path", "")
            
            # Create issue prompt generator config
            issue_config = {
                "external_program_path": external_program_path,
                "capture_before_after": self.config.get("defect_reporting", {}).get("capture_before_after", False),
                "capture_sequence": self.config.get("defect_reporting", {}).get("capture_sequence", False),
                "annotate_screenshots": self.config.get("defect_reporting", {}).get("annotate_screenshots", True),
                "severity_mapping": {
                    "external_crash": IssueSeverity.CRITICAL,
                    "external_error": IssueSeverity.ERROR,
                    "cus_failure": IssueSeverity.ERROR,
                    "ocr_mismatch": IssueSeverity.WARNING,
                    "timeout": IssueSeverity.WARNING
                }
            }
            
            self.issue_prompt_generator = IssuePromptGenerator(issue_config)
            print(f"IssuePromptGenerator initialized for: {external_program_path}")
            
        except Exception as e:
            print(f"Error setting up IssuePromptGenerator: {e}")
            self.issue_prompt_generator = None
    
    def load_test_sequences(self, filename: str = "test_sequences.json") -> bool:
        """Load test sequences from file"""
        try:
            print(f"Loading test sequences from {filename}")
            with open(filename, 'r') as f:
                sequences_data = json.load(f)
            
            self.sequences = []
            for seq_data in sequences_data:
                # Convert steps to ExecutionStep objects
                steps = []
                for step_data in seq_data.get("steps", []):
                    step = ExecutionStep(
                        trigger=step_data.get("trigger", ""),
                        action=step_data.get("action", ""),
                        description=step_data.get("description", ""),
                        expected_result=step_data.get("expected_result"),
                        timeout=step_data.get("timeout", 30)
                    )
                    steps.append(step)
                
                # Create SequenceExecution object
                sequence = SequenceExecution(
                    name=seq_data.get("name", ""),
                    description=seq_data.get("description", ""),
                    steps=steps,
                    priority=seq_data.get("priority", 1),
                    prerequisites=seq_data.get("prerequisites", [])
                )
                
                self.sequences.append(sequence)
            
            print(f"Loaded {len(self.sequences)} test sequences")
            self.metrics.total_sequences = len(self.sequences)
            self.metrics.total_steps = sum(len(seq.steps) for seq in self.sequences)
            
            return True
            
        except Exception as e:
            print(f"Error loading test sequences: {e}")
            return False
    
    def create_execution_plan(self) -> List[SequenceExecution]:
        """Create an optimized execution plan based on priorities and dependencies"""
        print("Creating execution plan...")
        
        # Sort sequences by priority (1=highest, 3=lowest)
        sorted_sequences = sorted(self.sequences, key=lambda x: x.priority)
        
        # Group by priority
        execution_plan = []
        for priority in [1, 2, 3]:
            priority_sequences = [seq for seq in sorted_sequences if seq.priority == priority]
            
            # Within same priority, randomize to avoid patterns
            random.shuffle(priority_sequences)
            execution_plan.extend(priority_sequences)
        
        print(f"Execution plan created with {len(execution_plan)} sequences")
        return execution_plan
    
    def execute_sequence(self, sequence: SequenceExecution) -> bool:
        """Execute a single test sequence"""
        print(f"=== EXECUTING SEQUENCE: {sequence.name} ===")
        
        sequence.state = SequenceState.RUNNING
        sequence.start_time = datetime.now()
        sequence.current_step = 0
        self.current_sequence = sequence
        
        try:
            for i, step in enumerate(sequence.steps):
                if self.stop_event.is_set():
                    print("Stop event received, aborting sequence")
                    sequence.state = SequenceState.FAILED
                    return False
                
                # Wait if paused
                if self.pause_event.is_set():
                    print("Execution paused")
                    sequence.state = SequenceState.PAUSED
                    self.pause_event.wait()
                    sequence.state = SequenceState.RUNNING
                
                sequence.current_step = i
                print(f"Step {i+1}/{len(sequence.steps)}: {step.description}")
                
                # Execute step
                step_result = self._execute_step(step)
                sequence.results.append(step_result)
                
                if step_result == StepResult.SUCCESS:
                    self.metrics.successful_steps += 1
                    sequence.coverage_achieved.add(step.trigger)
                elif step_result == StepResult.FAILED:
                    self.metrics.failed_steps += 1
                    error_msg = f"Step {i+1} failed: {step.description}"
                    sequence.error_messages.append(error_msg)
                    
                    if self.config["execution_settings"]["pause_on_error"]:
                        print(f"Pausing due to error: {error_msg}")
                        self.pause_execution()
                    
                    # Retry logic
                    if (self.config["execution_settings"]["retry_failed_steps"] and 
                        step.retry_count < step.max_retries):
                        print(f"Retrying step {i+1} (attempt {step.retry_count + 1})")
                        step.retry_count += 1
                        i -= 1  # Retry current step
                        continue
                
                # Add delay between steps if configured
                if self.config["execution_settings"]["adaptive_timing"]:
                    delay = random.uniform(0.5, 2.0)
                    time.sleep(delay)
            
            # Sequence completed
            sequence.state = SequenceState.COMPLETED
            sequence.end_time = datetime.now()
            self.metrics.completed_sequences += 1
            
            print(f"Sequence {sequence.name} completed successfully")
            return True
            
        except Exception as e:
            sequence.state = SequenceState.FAILED
            sequence.end_time = datetime.now()
            sequence.error_messages.append(f"Sequence failed: {str(e)}")
            self.metrics.failed_sequences += 1
            print(f"Sequence {sequence.name} failed: {e}")
            return False
    
    def _execute_step(self, step: ExecutionStep) -> StepResult:
        """Execute a single step in a sequence"""
        try:
            # This is where we would integrate with CUS to perform the actual action
            if self.cus_integration and self.cus_integration["available"]:
                result = self._execute_step_with_cus(step)
            else:
                result = self._execute_step_simulation(step)
            
            return result
            
        except Exception as e:
            print(f"Error executing step: {e}")
            return StepResult.FAILED
    
    def _execute_step_with_cus(self, step: ExecutionStep) -> StepResult:
        """Execute step using CUS integration"""
        # This would integrate with the actual CUS system
        # For now, we'll simulate the execution
        print(f"CUS: Trigger='{step.trigger}', Action='{step.action}'")
        
        # Simulate execution time
        time.sleep(random.uniform(0.5, 2.0))
        
        # Simulate success/failure based on realistic conditions
        if random.random() < 0.9:  # 90% success rate
            return StepResult.SUCCESS
        else:
            # Handle failure and generate defect prompt
            if self.current_sequence:
                self._handle_step_failure(
                    step, 
                    self.current_sequence, 
                    FailureType.CUS_SIMULATION_FAILURE,
                    {"error": "CUS simulation failed", "step": step.description}
                )
            return StepResult.FAILED
    
    def _execute_step_simulation(self, step: ExecutionStep) -> StepResult:
        """Execute step in simulation mode"""
        print(f"SIMULATION: Trigger='{step.trigger}', Action='{step.action}'")
        
        # Simulate execution time
        time.sleep(random.uniform(0.1, 0.5))
        
        # Always succeed in simulation mode
        return StepResult.SUCCESS
    
    def run_all_sequences(self) -> bool:
        """Run all loaded test sequences"""
        print("=== STARTING SEQUENCE EXECUTION ===")
        
        if not self.sequences:
            print("No sequences loaded")
            return False
        
        self.is_running = True
        execution_plan = self.create_execution_plan()
        
        start_time = time.time()
        
        try:
            for sequence in execution_plan:
                if self.stop_event.is_set():
                    print("Stop event received, aborting execution")
                    break
                
                # Check prerequisites
                if not self._check_prerequisites(sequence):
                    print(f"Prerequisites not met for {sequence.name}, skipping")
                    sequence.state = SequenceState.SKIPPED
                    self.metrics.skipped_sequences += 1
                    continue
                
                # Execute sequence
                success = self.execute_sequence(sequence)
                
                if not success and self.config["execution_settings"]["pause_on_error"]:
                    print("Pausing execution due to sequence failure")
                    self.pause_execution()
            
            # Calculate final metrics
            end_time = time.time()
            self.metrics.total_execution_time = end_time - start_time
            
            # Calculate coverage
            total_coverage_points = sum(len(seq.coverage_achieved) for seq in self.sequences)
            possible_coverage_points = sum(len(seq.steps) for seq in self.sequences)
            if possible_coverage_points > 0:
                self.metrics.coverage_percentage = (total_coverage_points / possible_coverage_points) * 100
            
            print("=== EXECUTION COMPLETED ===")
            self._print_execution_summary()
            
            return True
            
        except Exception as e:
            print(f"Error during execution: {e}")
            return False
        finally:
            self.is_running = False
    
    def _check_prerequisites(self, sequence: SequenceExecution) -> bool:
        """Check if prerequisites for a sequence are met"""
        if not sequence.prerequisites:
            return True
        
        # Check if prerequisite sequences have been completed
        for prereq in sequence.prerequisites:
            prereq_sequence = next((seq for seq in self.sequences if seq.name == prereq), None)
            if not prereq_sequence or prereq_sequence.state != SequenceState.COMPLETED:
                return False
        
        return True
    
    def pause_execution(self):
        """Pause execution"""
        print("Pausing execution...")
        self.pause_event.set()
    
    def resume_execution(self):
        """Resume execution"""
        print("Resuming execution...")
        self.pause_event.clear()
    
    def stop_execution(self):
        """Stop execution"""
        print("Stopping execution...")
        self.stop_event.set()
    
    def get_execution_status(self) -> Dict:
        """Get current execution status"""
        status = {
            "is_running": self.is_running,
            "is_paused": self.pause_event.is_set(),
            "current_sequence": self.current_sequence.name if self.current_sequence else None,
            "current_step": self.current_sequence.current_step if self.current_sequence else 0,
            "total_sequences": len(self.sequences),
            "completed_sequences": self.metrics.completed_sequences,
            "failed_sequences": self.metrics.failed_sequences,
            "skipped_sequences": self.metrics.skipped_sequences,
            "coverage_percentage": self.metrics.coverage_percentage
        }
        return status
    
    def _print_execution_summary(self):
        """Print execution summary"""
        print("\n=== EXECUTION SUMMARY ===")
        print(f"Total Sequences: {self.metrics.total_sequences}")
        print(f"Completed: {self.metrics.completed_sequences}")
        print(f"Failed: {self.metrics.failed_sequences}")
        print(f"Skipped: {self.metrics.skipped_sequences}")
        print(f"Total Steps: {self.metrics.total_steps}")
        print(f"Successful Steps: {self.metrics.successful_steps}")
        print(f"Failed Steps: {self.metrics.failed_steps}")
        print(f"Execution Time: {self.metrics.total_execution_time:.2f} seconds")
        print(f"Coverage: {self.metrics.coverage_percentage:.1f}%")
        
        # Show failed sequences
        failed_sequences = [seq for seq in self.sequences if seq.state == SequenceState.FAILED]
        if failed_sequences:
            print(f"\nFailed Sequences ({len(failed_sequences)}):")
            for seq in failed_sequences:
                print(f"  - {seq.name}: {seq.error_messages[-1] if seq.error_messages else 'Unknown error'}")
    
    def generate_html_report(self, filename: str = "execution_report.html"):
        """Generate HTML execution report"""
        try:
            html_content = self._create_html_report()
            with open(filename, 'w') as f:
                f.write(html_content)
            print(f"HTML report generated: {filename}")
        except Exception as e:
            print(f"Error generating HTML report: {e}")
    
    def _create_html_report(self) -> str:
        """Create HTML report content"""
        html = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <title>Test Sequence Execution Report</title>
            <style>
                body {{ font-family: Arial, sans-serif; margin: 20px; }}
                .header {{ background-color: #f0f0f0; padding: 20px; }}
                .metrics {{ display: flex; justify-content: space-around; margin: 20px 0; }}
                .metric {{ text-align: center; padding: 10px; background-color: #e0e0e0; }}
                .sequence {{ margin: 10px 0; padding: 10px; border: 1px solid #ccc; }}
                .success {{ background-color: #d4edda; }}
                .failed {{ background-color: #f8d7da; }}
                .skipped {{ background-color: #fff3cd; }}
                .steps {{ margin-top: 10px; }}
                .step {{ margin: 5px 0; padding: 5px; font-size: 0.9em; }}
            </style>
        </head>
        <body>
            <div class="header">
                <h1>Test Sequence Execution Report</h1>
                <p>Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
            </div>
            
            <div class="metrics">
                <div class="metric">
                    <h3>{self.metrics.total_sequences}</h3>
                    <p>Total Sequences</p>
                </div>
                <div class="metric">
                    <h3>{self.metrics.completed_sequences}</h3>
                    <p>Completed</p>
                </div>
                <div class="metric">
                    <h3>{self.metrics.failed_sequences}</h3>
                    <p>Failed</p>
                </div>
                <div class="metric">
                    <h3>{self.metrics.coverage_percentage:.1f}%</h3>
                    <p>Coverage</p>
                </div>
            </div>
            
            <h2>Sequence Details</h2>
        """
        
        # Add sequence details
        for seq in self.sequences:
            status_class = seq.state.value
            html += f"""
            <div class="sequence {status_class}">
                <h3>{seq.name}</h3>
                <p>{seq.description}</p>
                <p><strong>State:</strong> {seq.state.value}</p>
                <p><strong>Priority:</strong> {seq.priority}</p>
                <p><strong>Steps:</strong> {len(seq.steps)}</p>
                <p><strong>Coverage:</strong> {len(seq.coverage_achieved)} points</p>
                
                <div class="steps">
                    <h4>Steps:</h4>
            """
            
            for i, step in enumerate(seq.steps):
                result = seq.results[i] if i < len(seq.results) else "Not executed"
                html += f"""
                    <div class="step">
                        {i+1}. {step.description} - <em>{result}</em>
                    </div>
                """
            
            html += "</div>"
            
            if seq.error_messages:
                html += "<h4>Errors:</h4><ul>"
                for error in seq.error_messages:
                    html += f"<li>{error}</li>"
                html += "</ul>"
            
            html += "</div>"
        
        html += """
            </body>
        </html>
        """
        
        return html
    
    def save_execution_log(self, filename: str = "execution_log.json"):
        """Save detailed execution log"""
        try:
            log_data = {
                "timestamp": datetime.now().isoformat(),
                "metrics": {
                    "total_sequences": self.metrics.total_sequences,
                    "completed_sequences": self.metrics.completed_sequences,
                    "failed_sequences": self.metrics.failed_sequences,
                    "skipped_sequences": self.metrics.skipped_sequences,
                    "total_steps": self.metrics.total_steps,
                    "successful_steps": self.metrics.successful_steps,
                    "failed_steps": self.metrics.failed_steps,
                    "total_execution_time": self.metrics.total_execution_time,
                    "coverage_percentage": self.metrics.coverage_percentage
                },
                "sequences": []
            }
            
            for seq in self.sequences:
                seq_data = {
                    "name": seq.name,
                    "description": seq.description,
                    "state": seq.state.value,
                    "priority": seq.priority,
                    "start_time": seq.start_time.isoformat() if seq.start_time else None,
                    "end_time": seq.end_time.isoformat() if seq.end_time else None,
                    "steps": len(seq.steps),
                    "results": [result.value for result in seq.results],
                    "error_messages": seq.error_messages,
                    "coverage_achieved": list(seq.coverage_achieved)
                }
                log_data["sequences"].append(seq_data)
            
            with open(filename, 'w') as f:
                json.dump(log_data, f, indent=2)
            
            print(f"Execution log saved: {filename}")
            
        except Exception as e:
            print(f"Error saving execution log: {e}")
    
    def _handle_step_failure(self, step: ExecutionStep, sequence: SequenceExecution, 
                             failure_type: FailureType, error_details: Dict = None) -> None:
        """Handle step failure and generate defect prompt if available"""
        if not self.issue_prompt_generator:
            return
            
        try:
            # Create test case context
            test_context = TestCaseContext(
                test_case_name=sequence.name,
                test_sequence_id=sequence.sequence_id,
                expected_behavior=step.expected_result or "Step should complete successfully",
                actual_behavior=f"Step failed: {step.description}",
                failure_step=sequence.current_step,
                reproduction_steps=[s.description for s in sequence.steps[:sequence.current_step+1]],
                documentation_refs=[],  # Would be populated from test case metadata
                related_test_cases=[],
                dependency_chain=[]
            )
            
            # Create error context
            error_context = {
                "failure_type": failure_type.value,
                "error_details": error_details or {},
                "severity": self._determine_severity(failure_type).value,
                "step_info": {
                    "trigger": step.trigger,
                    "action": step.action,
                    "description": step.description
                }
            }
            
            # Generate defect prompt
            issue_prompt = self.issue_prompt_generator.generate_issue_prompt(
                test_case_context=test_context,
                error_context=error_context
            )
            
            print(f"Generated defect prompt: {issue_prompt.issue_id}")
            
        except Exception as e:
            print(f"Error generating defect prompt: {e}")
    
    def _determine_severity(self, failure_type: FailureType) -> IssueSeverity:
        """Determine issue severity based on failure type"""
        severity_mapping = {
            FailureType.EXTERNAL_PROGRAM_CRASH: IssueSeverity.CRITICAL,
            FailureType.EXTERNAL_PROGRAM_ERROR: IssueSeverity.ERROR,
            FailureType.CUS_SIMULATION_FAILURE: IssueSeverity.ERROR,
            FailureType.OCR_MISMATCH: IssueSeverity.WARNING,
            FailureType.TIMEOUT: IssueSeverity.WARNING,
            FailureType.NAVIGATION_FAILURE: IssueSeverity.ERROR,
            FailureType.VALIDATION_FAILURE: IssueSeverity.ERROR
        }
        return severity_mapping.get(failure_type, IssueSeverity.ERROR)

def main():
    """Main function for standalone execution"""
    print("=== SequenceRunner Standalone Execution ===")
    
    runner = SequenceRunner()
    
    # Load test sequences
    if not runner.load_test_sequences():
        print("Failed to load test sequences")
        return
    
    # Interactive menu
    while True:
        print("\n=== SEQUENCE RUNNER MENU ===")
        print("1. View loaded sequences")
        print("2. Run all sequences")
        print("3. Run specific sequence")
        print("4. View execution status")
        print("5. Generate HTML report")
        print("6. Save execution log")
        print("7. Exit")
        
        choice = input("Enter your choice (1-7): ").strip()
        
        if choice == "1":
            print(f"\nLoaded {len(runner.sequences)} sequences:")
            for i, seq in enumerate(runner.sequences):
                print(f"{i+1}. {seq.name} - {seq.description} (Priority: {seq.priority})")
        
        elif choice == "2":
            print("\nStarting execution of all sequences...")
            runner.run_all_sequences()
        
        elif choice == "3":
            print("\nAvailable sequences:")
            for i, seq in enumerate(runner.sequences):
                print(f"{i+1}. {seq.name}")
            
            try:
                seq_num = int(input("Enter sequence number: ")) - 1
                if 0 <= seq_num < len(runner.sequences):
                    runner.execute_sequence(runner.sequences[seq_num])
                else:
                    print("Invalid sequence number")
            except ValueError:
                print("Please enter a valid number")
        
        elif choice == "4":
            status = runner.get_execution_status()
            print("\n=== EXECUTION STATUS ===")
            for key, value in status.items():
                print(f"{key}: {value}")
        
        elif choice == "5":
            runner.generate_html_report()
        
        elif choice == "6":
            runner.save_execution_log()
        
        elif choice == "7":
            print("Exiting...")
            break
        
        else:
            print("Invalid choice. Please try again.")

if __name__ == "__main__":
    main()
