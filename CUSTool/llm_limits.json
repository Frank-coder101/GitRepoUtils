{
  "gpt-4o": {
    "max_tokens": 128000,
    "max_bytes": 400000,
    "recommended_prompt_bytes": 350000
  },
  "gpt-4-1106-preview": {
    "max_tokens": 128000,
    "max_bytes": 400000,
    "recommended_prompt_bytes": 350000
  },
  "gpt-4": {
    "max_tokens": 8192,
    "max_bytes": 32000,
    "recommended_prompt_bytes": 28000
  },
  "claude-3-sonnet-20240229": {
    "max_tokens": 200000,
    "max_bytes": 800000,
    "recommended_prompt_bytes": 700000
  },
  "claude-3-opus-20240229": {
    "max_tokens": 200000,
    "max_bytes": 800000,
    "recommended_prompt_bytes": 700000
  },
  "claude-3-haiku-20240307": {
    "max_tokens": 200000,
    "max_bytes": 800000,
    "recommended_prompt_bytes": 700000
  },
  "llama-3-70b": {
    "max_tokens": 8192,
    "max_bytes": 32000,
    "recommended_prompt_bytes": 28000
  },
  "llama-2-70b": {
    "max_tokens": 4096,
    "max_bytes": 16000,
    "recommended_prompt_bytes": 14000
  },
  "mistral-large": {
    "max_tokens": 32000,
    "max_bytes": 128000,
    "recommended_prompt_bytes": 110000
  },
  "gemini-1.5-pro": {
    "max_tokens": 1000000,
    "max_bytes": 4000000,
    "recommended_prompt_bytes": 3500000
  }
}
