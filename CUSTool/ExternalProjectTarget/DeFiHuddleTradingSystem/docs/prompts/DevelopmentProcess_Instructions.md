# Development Process Instructions for AI (Copilot)

# Automation Directive For AI (Copilot)
1) AI (Copilot) is not authorized to change or delete this file
2) All steps in this process are to be performed automatically and sequentially by the AI.
3) All steps are to be performed without awaiting user input.
4) If a decision or assumption is required, the AI will estimate the best course of action, document the assumption in docs/DevelopmentProcessAssumptionsLog.md (create it if it does not exist), and proceed.
5) Only stop if explicitly instructed by the user.
6) Never use `venv`, this has always resulted in endless error traps for GitHub Copilot. Instead run test on the main python environment.

# Overview
This document defines the detailed, top-down, requirements-driven process for generating production-ready code, architecture, and tests using AI. All logic within this file (and all files it refers to for criteria-based analysis and determinations) is to be interpreted and executed by the AI assistant (e.g., Copilot). 

# Key Principles
- Avoid placeholder, mock, or stub code; always aim for production-ready implementations.
- Ensure traceability from requirements → architecture → code → tests, updating traceability matrices as needed.
- Document all assumptions and actions in `docs/AssumptionsLog.md` for traceability.
- Apply the criteria for sufficiency and completeness at every step.
- Maximum Recursion Depth: enable a protection of maximum 500 levels of recursion depth for all the instructions in this file
- Log any ambiguity and assumptions made during development process execution in /docs/autogenerated/development_process_execution_log.md

# Process Steps

### Initialize
Load the `requirement analysis criteria` from docs/prompts/DevelopmentProcess_Criteria_Requirement.md
Load the `artifact analysis criteria` from docs/prompts/DevelopmentProcess_Criteria_Artifact.md
Load the `code file analysis criteria` from docs/prompts/DevelopmentProcess_Criteria_CodeFile.md

### Start Process
For Each top-level requirement in the docs/Project_Requirements.md file:
    Perform Step `1. Requirements Decomposition and Detailing`
Next

### 1. Requirements Decomposition and Detailing
    - Analyze if the requirement meets the `requirement analysis criteria`'s and if child requirements are needed.
    - If requirement does not meet `requirement analysis criteria` 
        - Upgrade the requirement content to be actionable and unambiguous
        - Update it in the docs/Project_Requirements.md file
    - End If
    - Perform Step `2. Architecture Artifact Generation` WITH THIS Requirement
    - For Each missing child requirement
        - Create the requirement using the `requirement analysis criteria` as a guideline
        - Add it to the docs/Project_Requirements.md file, using sub-numbering and placing them below the parent.
    - Next
    - For Every child requirements ("every" includes those created a few lines above)
        - Recursively Repeat Performing Step `1. Requirements Decomposition and Detailing`
    - Next

### 2. Architecture Artifact Generation (WITH GIVEN Requirement)
    - If there no list of related artifacts in the docs/autogenerated/Requirements Traceability Matrix.csv file
        - Create the artifact using the `artifact analysis criteria` as a guideline
        - Add it in the docs/Architecture_Document.md file below its parent, or after the last existing child section
        - Update the Requirements Traceability Matrix accordingly WITH GIVEN Requirement(add a row if one does not exist, with all needed columns).
        - Perform Step `3. Code File Generation and Analysis` WITH THIS artifact
        - For Each missing child artifacts needed
            - Create the artifact using the `artifact analysis criteria` as a guideline
            - Add it to the docs/Architecture_Document.md file next to the parent artifact section, or after the last existing child in the section, adding information identifying the parent artifact
        - Next
    - Else
        - For Each artifact in the list
            - Analyze if an existing artifact fulfills the related requirements, and meets `artifact analysis criteria`, and if child artifacts are missing from the artifact's location in the docs/Architecture_Document.md
            - If the artifact does not meet the 'artifact analysis criteria'
                Upgrade the artifact content to be actionable and unambiguous 
                Update it in the docs/Architecture_Document.md file
            - End If
            - Perform Step `3. Code File Generation and Analysis` WITH THIS artifact
            - For Each missing child artifacts needed
                - Create the artifact using the `artifact analysis criteria` as a guideline
                - Add it to the docs/Architecture_Document.md file next to the parent artifact section, or after the last existing child in the section, adding information identifying the parent artifact
            - Next
        - Next
    - End If
    
### 3. Code File Generation and Analysis (WITH GIVEN Artifact)
    - Using GIVEN Artifact, get a list of associated code files from the Artifact Traceability Matrix.
    - Search the project directory and subdirectories for any source code files that implement the artifact ("ghost files"). Add these to the associated code file list and update the `Artifact Traceability Matrix` as needed with GIVEN Artifact.
    - For each code file in the code file list:
        - Analyze the code file correctly implements the specifications in the GIVEN Artifact, meets the `code file analysis criteria`, and if child artifacts are missing from the docs/autogenerated/Code_File_Dependency_Matrix.md file
        - If the code file does not correctly implement the specifications in an artifact or does not meet meet the `code file analysis criteria`
            - Upgrade and optimize the code file using the `code file analysis criteria` as a guidline.
        - End If
        - For Each missing child code file in docs/autogenerated/Code_File_Dependency_Matrix.md file
            - Create the missing child code file using the `code file analysis criteria` as a guideline and information identifying the parent code file
            - Place the missing child code file at the appropriate project folder tree location -either at the same folder of the parent code file, or a sub folder, adding information identifying the child code file and parent code file in the in docs/autogenerated/Code_File_Dependency_Matrix.md file (with appropriate columns)
        - Next
    - Next

### 4. Testing 

#### `TestModes` Definition
- There are 2 Test Modes: "unit" and "integration".
Unit TestMode: this is the default mode to run all the tests in, and is always the first test run. Resolve all defects and repete another "unit" TestMode run until all tests pass.
- Integration TestMode: in this mode, the code for all integrations to other systems has to be executed. Placeholder, mock, or stub code is strictly forbidden to execute and must be monitored. If detected, the test run must abort, and the offending code must be ugraded to continue supporting "unit" test mode, but ensure that when "integration" mode test runs execute in the future, the full implementation of the integration (APIs) is executed and tested -even if errors occur. In fact, the first "integration" test run will always ensure connectivity is not available and that run time errors are raised to avoid `false negative` test results defined below.

#### `False Negatives` Test Anti Pattern in Integration and Unit TestModes

#### Context
- In integration and unit testing, a common anti-pattern is when a test always passes (false negative) because the code under test returns a success value (e.g., `True`) regardless of whether the integration or logic actually succeeded.
- This can mask real integration failures and give a false sense of reliability.

#### Examples Found in Project
- `OrderManager.cancel_order` previously always returned `True` in integration mode, even if the broker was not connected. This caused tests to pass even when IBKR was unavailable.
- `RiskEngine.check_risk` always returned `True` (stub), so tests using `assertTrue(result)` would always pass, even though no real risk logic was implemented.

#### Corrective Actions
- `OrderManager.cancel_order` now checks broker connectivity and IBKR availability in integration mode, raising an exception if not connected, matching the logic in `place_order`.
- `RiskEngine.check_risk` now raises `NotImplementedError` in integration mode until real logic is implemented.
- Corresponding tests have been updated to expect these failures in integration mode (using `assertRaises`).

#### IBKR Connectivity Testing and Corrective Actions
- `BrokerManager.place_order` now includes retry logic and detailed error handling for IBKR API failures.
- `BrokerManager.get_account_data` retrieves and parses account information with robust error handling.
- `OrderManager` methods (`place_order`, `cancel_order`) validate broker connectivity and handle exceptions gracefully.
- Integration tests for IBKR connectivity validate order placement, account data retrieval, and error scenarios.

#### Test Discovery Note
- To run integration tests, set the appropriate environment variable (e.g., `TEST_MODE=integration`) before running pytest, or update test code to accept a CLI flag if needed.
- Always running test commands from the `DeFiHuddleTradingSystem` directory.
- Detect the required working directory and output paths.
- Check and create the `docs/autogenerated` directory if it does not exist.
- If test runs report "NO TESTS RAN," check for test discovery issues (e.g., missing `__init__.py`, test file/class/method naming).
- The logic is correct when integration tests will fail if integration is not implemented or the broker is unavailable.


#### Guidance
- Always ensure that integration and unit test logic does not return unconditional success in any mode where real integration or logic is required.
- Use exceptions and explicit failure modes to ensure tests fail when integration is not available or not implemented.

#### Creating Test Code
For each code file:
  - Update or create the test file ensuring the test code differentiates between the `TestModes`
  - Ensure the code coverage of the code file is as practically optimal as possible
  - Eliminate all `false negatives` anti-pattern, and do not create placeholder, mock, or stub code; always aim for production-ready implementations.

#### Conducting Test Runs
##### Conducting `Unit` Test Runs
Conduct full "unit" test runs and fix all defects in the code files, or the test code -which ever will yield the higher production ready code, after each test run. Repeat until all tests pass for a given run. 
Report the code coverage for the passing test run in the `docs/autogenerated/CodeCoverageLog.md` file with (test end time, duration, testmode, coverage)

If there are practical opportunities to enhance code coverage, implement related test code enhancements and repeat section "##### Conducting `Unit` Test Runs"

##### Conducting `Integration` Test Runs
Proceed to execute test runs in "integration" testmode while tracking and eliminating all:
- `false negatives` anti-pattern
- Placeholder, mock, or stub code
Stop doing "integration" test runs when the code coverage is the same or higher as the last passing "unit" test run code coverage.
Report the code coverage for the passing test run in the `docs/autogenerated/CodeCoverageLog.md` file with (test end time, duration, testmode, coverage)

##### Integration Guide Generation 

Review all current project files and generate a step-by-step, actionable guide for a technical operator to configure and authenticate every integration (e.g., broker APIs, Google Drive, OpenAI, database) so that all integration tests will pass. 
- This guide will be called IntegrationsConfigurationGuide.md
- This guide will be place in the docs/autogenerated folder
- Include exact configuration file locations, required environment variables, credential setup, and any prerequisite software or services. 
    If exact locations are not known, search from the top folder 
- For each integration, specify:
  - What must be installed or running and the instructions
        Include any web link if useful
  - What credentials or API keys are needed and where to place them
  - How to verify the integration is working
  - Any troubleshooting tips for common errors
- Output the instructions in a checklist format, with file paths and example config snippets where possible.
- Update this guide whenever integrations, configuration, or test requirements change.

### 5. Retrospective
Create a report file in docs/autogenerated/retrospectives. 
    The file name will be different each time and will follow this string format "RetrospectiveReport_"+ (Full day of the week) +"_"+ (AI must generate a globally unique identifier to be appended to the file name)

Analyze all the project files in /DeFiHuddleTradingSystem (and all sub folders) and determine what needs to be improved and what needs to be eliminated for the following activities and outcomes:
1) How the process what executed
2) How all the changes in requirements, artifacts and code files were implemented
3) What standsout from the recorded assumptions and the other logs
4) How are all the autogenerated data administered and used for further improvements
5) How were the test runs conducted
6) How were the prompts between the AI and the user

Present only 
a) what needs to be improved and
b) what needs to be eliminated 

There is no need to go over the details of what was done as expected
